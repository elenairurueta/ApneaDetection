homepap-lab-full-1600043

Subset 0 data count: 95
	With apnea: 4
	Without apnea: 91
Subset 1 data count: 94
	With apnea: 4
	Without apnea: 90
Subset 2 data count: 91
	With apnea: 3
	Without apnea: 88
Subset 3 data count: 94
	With apnea: 4
	Without apnea: 90
Subset 4 data count: 98
	With apnea: 4
	Without apnea: 94
Subset 5 data count: 95
	With apnea: 3
	Without apnea: 92
Subset 6 data count: 95
	With apnea: 4
	Without apnea: 91
Subset 7 data count: 95
	With apnea: 3
	Without apnea: 92
Subset 8 data count: 95
	With apnea: 4
	Without apnea: 91
Subset 9 data count: 95
	With apnea: 3
	Without apnea: 92
Train: subsets [0, 1, 2, 3, 4, 5, 6, 7]
Val: subset [8]
Test: subset [9]

UNDERSAMPLING

Subset 0 data count: 8
	With apnea: 4
	Without apnea: 4
Subset 1 data count: 8
	With apnea: 4
	Without apnea: 4
Subset 2 data count: 6
	With apnea: 3
	Without apnea: 3
Subset 3 data count: 8
	With apnea: 4
	Without apnea: 4
Subset 4 data count: 8
	With apnea: 4
	Without apnea: 4
Subset 5 data count: 6
	With apnea: 3
	Without apnea: 3
Subset 6 data count: 8
	With apnea: 4
	Without apnea: 4
Subset 7 data count: 6
	With apnea: 3
	Without apnea: 3
Subset 8 data count: 8
	With apnea: 4
	Without apnea: 4
Subset 9 data count: 95
	With apnea: 3
	Without apnea: 92

Model2(
  (conv_layers): Sequential(
    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))
    (1): ReLU()
    (2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (3): ReLU()
    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Dropout(p=0.5, inplace=False)
    (7): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
    (8): ReLU()
    (9): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
    (10): ReLU()
    (11): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (13): Dropout(p=0.5, inplace=False)
    (14): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
    (15): ReLU()
    (16): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
    (17): ReLU()
    (18): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (19): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (20): Dropout(p=0.5, inplace=False)
  )
  (fc_layers): Sequential(
    (0): Linear(in_features=245760, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=1, bias=True)
    (5): Sigmoid()
  )
)

Batch size: 32

Loss function: BCELoss()

Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)

Start of training
	End of epoch 1 - Accuracy = 50.00% - F1 = 0.00% - Train Loss = 67.85% - Val Loss = 69.36% - 10.47 seconds
	End of epoch 2 - Accuracy = 50.00% - F1 = 0.00% - Train Loss = 51.54% - Val Loss = 69.48% - 9.73 seconds
End of training - 2 epochs - 49.57 seconds
Best model - Epoch 1 - Val Loss = 51.54%