homepap-lab-full-1600095

Subset 0 data count: 77
	With apnea: 5
	Without apnea: 72
Subset 1 data count: 76
	With apnea: 5
	Without apnea: 71
Subset 2 data count: 74
	With apnea: 5
	Without apnea: 69
Subset 3 data count: 76
	With apnea: 6
	Without apnea: 70
Subset 4 data count: 79
	With apnea: 6
	Without apnea: 73
Subset 5 data count: 77
	With apnea: 5
	Without apnea: 72
Subset 6 data count: 77
	With apnea: 6
	Without apnea: 71
Subset 7 data count: 76
	With apnea: 5
	Without apnea: 71
Subset 8 data count: 77
	With apnea: 6
	Without apnea: 71
Subset 9 data count: 76
	With apnea: 5
	Without apnea: 71
Train: subsets [0, 1, 2, 3, 4, 5, 6, 7]
Val: subset [8]
Test: subset [9]

UNDERSAMPLING

Subset 0 data count: 10
	With apnea: 5
	Without apnea: 5
Subset 1 data count: 10
	With apnea: 5
	Without apnea: 5
Subset 2 data count: 10
	With apnea: 5
	Without apnea: 5
Subset 3 data count: 12
	With apnea: 6
	Without apnea: 6
Subset 4 data count: 12
	With apnea: 6
	Without apnea: 6
Subset 5 data count: 10
	With apnea: 5
	Without apnea: 5
Subset 6 data count: 12
	With apnea: 6
	Without apnea: 6
Subset 7 data count: 10
	With apnea: 5
	Without apnea: 5
Subset 8 data count: 12
	With apnea: 6
	Without apnea: 6
Subset 9 data count: 76
	With apnea: 5
	Without apnea: 71

Model2(
  (conv_layers): Sequential(
    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))
    (1): ReLU()
    (2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (3): ReLU()
    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Dropout(p=0.5, inplace=False)
    (7): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
    (8): ReLU()
    (9): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
    (10): ReLU()
    (11): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (13): Dropout(p=0.5, inplace=False)
    (14): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
    (15): ReLU()
    (16): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
    (17): ReLU()
    (18): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (19): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (20): Dropout(p=0.5, inplace=False)
  )
  (fc_layers): Sequential(
    (0): Linear(in_features=192000, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=1, bias=True)
    (5): Sigmoid()
  )
)

Batch size: 32

Loss function: BCELoss()

Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)

Start of training
	End of epoch 1 - Accuracy = 50.00% - F1 = 66.67% - Train Loss = 67.99% - Val Loss = 69.32% - 10.15 seconds
	End of epoch 2 - Accuracy = 41.67% - F1 = 58.82% - Train Loss = 59.66% - Val Loss = 69.36% - 10.03 seconds
End of training - 2 epochs - 23.63 seconds
Best model - Epoch 1 - Val Loss = 69.32%