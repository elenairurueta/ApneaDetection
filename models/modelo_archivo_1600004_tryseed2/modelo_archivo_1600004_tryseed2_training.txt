homepap-lab-full-1600004

Subset 0 data count: 76
	With apnea: 8
	Without apnea: 68
Subset 1 data count: 75
	With apnea: 8
	Without apnea: 67
Subset 2 data count: 72
	With apnea: 7
	Without apnea: 65
Subset 3 data count: 74
	With apnea: 7
	Without apnea: 67
Subset 4 data count: 78
	With apnea: 8
	Without apnea: 70
Subset 5 data count: 76
	With apnea: 8
	Without apnea: 68
Subset 6 data count: 75
	With apnea: 7
	Without apnea: 68
Subset 7 data count: 75
	With apnea: 8
	Without apnea: 67
Subset 8 data count: 75
	With apnea: 8
	Without apnea: 67
Subset 9 data count: 75
	With apnea: 7
	Without apnea: 68
Train: subsets [0, 1, 2, 3, 4, 5, 6, 7]
Val: subset [8]
Test: subset [9]

UNDERSAMPLING

Subset 0 data count: 16
	With apnea: 8
	Without apnea: 8
Subset 1 data count: 16
	With apnea: 8
	Without apnea: 8
Subset 2 data count: 14
	With apnea: 7
	Without apnea: 7
Subset 3 data count: 14
	With apnea: 7
	Without apnea: 7
Subset 4 data count: 16
	With apnea: 8
	Without apnea: 8
Subset 5 data count: 16
	With apnea: 8
	Without apnea: 8
Subset 6 data count: 14
	With apnea: 7
	Without apnea: 7
Subset 7 data count: 16
	With apnea: 8
	Without apnea: 8
Subset 8 data count: 16
	With apnea: 8
	Without apnea: 8
Subset 9 data count: 75
	With apnea: 7
	Without apnea: 68

Model2(
  (conv_layers): Sequential(
    (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))
    (1): ReLU()
    (2): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))
    (3): ReLU()
    (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Dropout(p=0.5, inplace=False)
    (7): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))
    (8): ReLU()
    (9): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
    (10): ReLU()
    (11): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (13): Dropout(p=0.5, inplace=False)
    (14): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))
    (15): ReLU()
    (16): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
    (17): ReLU()
    (18): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (19): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (20): Dropout(p=0.5, inplace=False)
  )
  (fc_layers): Sequential(
    (0): Linear(in_features=245760, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=1, bias=True)
    (5): Sigmoid()
  )
)

Batch size: 32

Loss function: BCELoss()

Optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    fused: None
    lr: 0.01
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 0
)

Start of training
	End of epoch 1 - Accuracy = 50.00% - F1 = 0.00% - Train Loss = 62.30% - Val Loss = 69.25% - 20.04 seconds
	End of epoch 2 - Accuracy = 50.00% - F1 = 0.00% - Train Loss = 48.55% - Val Loss = 69.25% - 19.29 seconds
End of training - 2 epochs - 39.33 seconds